{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14427553,"sourceType":"datasetVersion","datasetId":9215275}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Dependencies","metadata":{}},{"cell_type":"code","source":"%%capture\n# 1. Force un-install potentially conflicting libraries first\n!pip uninstall -y unsloth unsloth-zoo peft trl transformers\n\n# 2. Install Unsloth and compatible dependencies\n# We use the specific 'colab-new' tag which is stable for T4 environments like Kaggle/Colab\n!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# 3. Install other requirements without deps to prevent version overwrites\n!pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes unsloth-zoo","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import Libraries and Setup","metadata":{}},{"cell_type":"code","source":"import torch\nfrom unsloth import FastLanguageModel\nimport json\nimport pandas as pd\nfrom datasets import Dataset, concatenate_datasets\nimport os\nimport glob\n\n# Check GPU capability\nmax_seq_length = 2048 # Auto supports RoPE Scaling\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage\n\nprint(f\"GPU Model: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load, Merge, and Count Data","metadata":{}},{"cell_type":"code","source":"# Define your specific file paths\nfile_paths = [\n    \"/kaggle/input/nusantara-law-corpus/Adagium/all.json\",\n    \"/kaggle/input/nusantara-law-corpus/Glosarium-MA/all.json\",\n    \"/kaggle/input/nusantara-law-corpus/KHPTSultra/all.json\",\n    \"/kaggle/input/nusantara-law-corpus/LawDictionary/all.json\",\n    \"/kaggle/input/nusantara-law-corpus/UUD/uud.json\"\n]\n\ncombined_data = []\n\n# Iterate and load\nfor file_path in file_paths:\n    if os.path.exists(file_path):\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                # Ensure data is a list of records\n                if isinstance(data, list):\n                    combined_data.extend(data)\n                    print(f\"Successfully loaded {len(data)} records from: {os.path.basename(file_path)}\")\n                else:\n                    print(f\"Warning: {file_path} format is not a list of records.\")\n        except Exception as e:\n            print(f\"Error reading {file_path}: {e}\")\n    else:\n        print(f\"File not found: {file_path}\")\n\n# Convert to Pandas DataFrame first for easier handling\ndf = pd.DataFrame(combined_data)\n\n# Print total count\nprint(\"-\" * 30)\nprint(f\"Total merged data points: {len(df)}\")\nprint(\"-\" * 30)\n\n# Convert to Hugging Face Dataset format\ndataset = Dataset.from_pandas(df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Model (LLama 3 8b)","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\n# Llama 3 8b (Great balance of speed and smarts)\nmodel_name = \"unsloth/llama-3-8b-bnb-4bit\"\n\nmax_seq_length = 2048\ndtype = None \nload_in_4bit = True \n\nprint(f\"Loading Model: {model_name}...\")\n\n# 1. Load Model & Tokenizer\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_name,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\n# 2. Configure Adapters (Works for all 3 models)\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, \n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)\n\n# 3. Define Formatting Function\n# We stick to Alpaca format because your dataset is built for it.\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token \n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    contexts     = examples[\"context\"]\n    responses    = examples[\"response\"]\n    texts = []\n    for instruction, context, response in zip(instructions, contexts, responses):\n        text = alpaca_prompt.format(instruction, context, response) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\n\n# 4. Apply Formatting\n# (Assuming 'dataset' is already loaded from previous cells)\ndataset = dataset.map(formatting_prompts_func, batched = True)\n\n# 5. Split Data (90% Train, 10% Test)\ndataset_split = dataset.train_test_split(test_size=0.1)\ntrain_dataset = dataset_split[\"train\"]\neval_dataset = dataset_split[\"test\"]\n\nprint(f\"Success! Loaded {model_name} and formatted data.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Configure QLoRA Adapters","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Rank. 16 is a good balance. 32/64 if you have a lot of data/compute.\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0, # 0 is optimized\n    bias = \"none\",    # \"none\" is optimized\n    use_gradient_checkpointing = \"unsloth\", # Crucial for long context/low VRAM\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training (SFTTrainer)","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\n# 1. Split data (90% train, 10% test)\ndataset_split = dataset.train_test_split(test_size=0.1)\ntrain_dataset = dataset_split[\"train\"]\neval_dataset = dataset_split[\"test\"]\n\n# 2. Configure Training for ~6 Hours\nsft_config = SFTConfig(\n    output_dir=\"outputs\",\n    max_seq_length=max_seq_length,\n    dataset_text_field=\"text\",\n    dataset_num_proc=2,\n    packing=False,\n    # Batch Size & Gradient Accumulation\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    # Duration\n    num_train_epochs=1, \n    # Learning Rate & Optimizer\n    learning_rate=5e-5,\n    lr_scheduler_type=\"cosine\",\n    warmup_steps=100,\n    optim=\"adamw_8bit\",\n    weight_decay=0.01,\n    # Precision (T4 uses FP16)\n    fp16=True,\n    bf16=False,\n    # Evaluation & Saving Strategy\n    eval_strategy=\"steps\",\n    eval_steps=175,        \n    save_steps=175,        \n    logging_steps=10,\n    # RISK MANAGEMENT\n    save_total_limit=1,          \n    load_best_model_at_end=True, \n    metric_for_best_model=\"eval_loss\", \n    greater_is_better=False,\n    report_to=\"none\",\n    seed=3407,\n)\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    eval_dataset = eval_dataset,   \n    args = sft_config,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Execute Training","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference and Evaluation","metadata":{}},{"cell_type":"code","source":"import time\n\n# 1. Switch to inference mode\nFastLanguageModel.for_inference(model)\n\n# 2. Define varied test cases from your dataset\ntest_prompts = [\n    \"Jelaskan prinsip 'Nullum delictum nulla poena sine praevia lege poenali'.\",\n    \"Apa sanksi bagi pejabat yang menyalahgunakan wewenang?\",\n    \"Jelaskan perbedaan antara hukum perdata dan hukum pidana.\"\n]\n\n# 3. Advanced Generation Function\ndef generate_response(prompt, context=\"\"):\n    inputs = tokenizer(\n        [\n            alpaca_prompt.format(\n                prompt,\n                context,\n                \"\", # Output is empty for generation\n            )\n        ],\n        return_tensors=\"pt\"\n    ).to(\"cuda\")\n\n    start_time = time.time()\n    \n    # Advanced parameters for control\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=256,       \n        use_cache=True,\n        temperature=0.6,          # Lower (0.1-0.6) is better for factual/legal tasks\n        top_k=50,                 \n        top_p=0.9,                \n        repetition_penalty=1.1,   \n        do_sample=True            \n    )\n    \n    end_time = time.time()\n    \n    decoded_output = tokenizer.batch_decode(outputs)[0]\n    response = decoded_output.split(\"### Response:\\n\")[-1].replace(tokenizer.eos_token, \"\")\n    \n    # Calculate speed\n    num_tokens = len(outputs[0])\n    duration = end_time - start_time\n    tokens_per_sec = num_tokens / duration\n    \n    return response, tokens_per_sec\n\n# 4. Run Evaluation\nprint(\"Starting Advanced Evaluation\\n\")\n\nfor i, prompt in enumerate(test_prompts):\n    print(f\"Test Case {i+1}: {prompt}\")\n    response, speed = generate_response(prompt)\n    print(f\"Response:\\n{response}\")\n    print(f\"Speed: {speed:.2f} tokens/sec\")\n    print(\"-\" * 50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cleanup Cell","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\n\n# Delete the 'outputs' folder to free up space for GGUF conversion\n# The model weights are already loaded in RAM, so these files are safe to delete now.\npath_to_clean = \"/kaggle/working/outputs\"\n\nif os.path.exists(path_to_clean):\n    print(f\"Cleaning up {path_to_clean} to prevent 'No space left on device' error...\")\n    try:\n        shutil.rmtree(path_to_clean)\n        print(\"Cleanup successful. Disk space reclaimed.\")\n    except Exception as e:\n        print(f\"Could not fully clean directory: {e}\")\nelse:\n    print(f\"Directory {path_to_clean} not found, skipping cleanup.\")\n\n# Verify available space\n!df -h /kaggle/working","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save the Model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# 1. Login\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_token\")\nlogin(hf_token)\n\nrepo_name = \"bayhaqieee/llama3-8b-nlaw-gguf\" \n\nprint(\"Pushing GGUF to Hugging Face (This takes time & RAM)\")\n# We do this FIRST to ensure disk space is available for the merge process.\ntry:\n    model.save_pretrained_gguf(\n        repo_name, \n        tokenizer, \n        quantization_method = \"q4_k_m\",\n        token = hf_token\n    )\n    print(\"GGUF Pushed to Hugging Face successfully!\")\nexcept Exception as e:\n    print(f\"GGUF Push Failed: {e}\")\n\nprint(\"\\n Pushing Adapters (LoRA) to Hugging Face \")\n# This saves just the small learning files to your repo\nmodel.push_to_hub(repo_name, token=hf_token)\ntokenizer.push_to_hub(repo_name, token=hf_token)\nprint(\"Adapters Pushed to Hugging Face!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nSaving Adapters Locally (Kaggle)\")\nlocal_folder = \"llama3-8b-nlaw_adapter\"\nmodel.save_pretrained(local_folder)\ntokenizer.save_pretrained(local_folder)\nprint(f\"Adapters saved locally to folder: {local_folder}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}