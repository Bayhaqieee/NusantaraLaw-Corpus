{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14427553,"sourceType":"datasetVersion","datasetId":9215275}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Install Dependencies","metadata":{}},{"cell_type":"code","source":"%%capture\n# 1. Force un-install potentially conflicting libraries first\n!pip uninstall -y unsloth unsloth-zoo peft trl transformers\n\n# 2. Install Unsloth and compatible dependencies\n# We use the specific 'colab-new' tag which is stable for T4 environments like Kaggle/Colab\n!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\n# 3. Install other requirements without deps to prevent version overwrites\n!pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes unsloth-zoo","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:02:46.265756Z","iopub.execute_input":"2026-01-10T15:02:46.266151Z","iopub.status.idle":"2026-01-10T15:03:11.667211Z","shell.execute_reply.started":"2026-01-10T15:02:46.266119Z","shell.execute_reply":"2026-01-10T15:03:11.666240Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Import Libraries and Setup","metadata":{}},{"cell_type":"code","source":"import torch\nfrom unsloth import FastLanguageModel\nimport json\nimport pandas as pd\nfrom datasets import Dataset, concatenate_datasets\nimport os\nimport glob\n\n# Check GPU capability\nmax_seq_length = 2048 # Auto supports RoPE Scaling\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage\n\nprint(f\"GPU Model: {torch.cuda.get_device_name(0)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:03:11.668877Z","iopub.execute_input":"2026-01-10T15:03:11.669238Z","iopub.status.idle":"2026-01-10T15:03:11.675009Z","shell.execute_reply.started":"2026-01-10T15:03:11.669177Z","shell.execute_reply":"2026-01-10T15:03:11.674185Z"}},"outputs":[{"name":"stdout","text":"GPU Model: Tesla T4\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## Load, Merge, and Count Data","metadata":{}},{"cell_type":"code","source":"# Define your specific file paths\nfile_paths = [\n    \"/kaggle/input/nusantara-law-corpus/Adagium/all.json\",\n    \"/kaggle/input/nusantara-law-corpus/Glosarium-MA/all.json\",\n    \"/kaggle/input/nusantara-law-corpus/KHPTSultra/all.json\",\n    \"/kaggle/input/nusantara-law-corpus/LawDictionary/all.json\",\n    \"/kaggle/input/nusantara-law-corpus/UUD/uud.json\"\n]\n\ncombined_data = []\n\n# Iterate and load\nfor file_path in file_paths:\n    if os.path.exists(file_path):\n        try:\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                # Ensure data is a list of records\n                if isinstance(data, list):\n                    combined_data.extend(data)\n                    print(f\"Successfully loaded {len(data)} records from: {os.path.basename(file_path)}\")\n                else:\n                    print(f\"Warning: {file_path} format is not a list of records.\")\n        except Exception as e:\n            print(f\"Error reading {file_path}: {e}\")\n    else:\n        print(f\"File not found: {file_path}\")\n\n# Convert to Pandas DataFrame first for easier handling\ndf = pd.DataFrame(combined_data)\n\n# Print total count\nprint(\"-\" * 30)\nprint(f\"Total merged data points: {len(df)}\")\nprint(\"-\" * 30)\n\n# Convert to Hugging Face Dataset format\ndataset = Dataset.from_pandas(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:03:11.675888Z","iopub.execute_input":"2026-01-10T15:03:11.676173Z","iopub.status.idle":"2026-01-10T15:03:11.728374Z","shell.execute_reply.started":"2026-01-10T15:03:11.676126Z","shell.execute_reply":"2026-01-10T15:03:11.727809Z"}},"outputs":[{"name":"stdout","text":"Successfully loaded 89 records from: all.json\nSuccessfully loaded 207 records from: all.json\nSuccessfully loaded 144 records from: all.json\nSuccessfully loaded 2456 records from: all.json\nSuccessfully loaded 250 records from: uud.json\n------------------------------\nTotal merged data points: 3146\n------------------------------\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Load Model (Gemma 2 9B)","metadata":{}},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/gemma-2-9b-bnb-4bit\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)\n\n# Now we apply the formatting function using the loaded tokenizer's EOS token\nEOS_TOKEN = tokenizer.eos_token\ndataset = dataset.map(formatting_prompts_func, batched = True)\n\n# Inspect one example to ensure it looks correct\nprint(dataset[0][\"text\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:03:11.730048Z","iopub.execute_input":"2026-01-10T15:03:11.730507Z","iopub.status.idle":"2026-01-10T15:03:51.131453Z","shell.execute_reply.started":"2026-01-10T15:03:11.730481Z","shell.execute_reply":"2026-01-10T15:03:51.130639Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2026.1.2: Fast Gemma2 patching. Transformers: 4.57.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.13G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3491075627ad4cdba83801a1d17b546a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf8d0629a3ae4df69058affe932e21b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9113b79c50e4e66aac7c380744bbfd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"735efb57bd4f4b2ab6ccc563cdd01109"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"066b6e927d2d403db498bd6c973784f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6b21d4660774e9bbdd532b10b6de8c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3146 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96c5e5bc87d64f968d4d81c8c67f2113"}},"metadata":{}},{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nApa arti dari dalil hukum 'Absolute sentienfia expositore non indiget'?\n\n### Input:\nIstilah Hukum / Adagium\n\n### Response:\nSebuah dalil yang sederhana tidak membutuhkan penjelasan lebih lanjut.<eos>\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Format Data for the Model","metadata":{}},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n\n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    contexts     = examples[\"context\"]\n    responses    = examples[\"response\"]\n    texts = []\n    for instruction, context, response in zip(instructions, contexts, responses):\n        # The global EOS_TOKEN will be available when this function is called later\n        text = alpaca_prompt.format(instruction, context, response) + EOS_TOKEN\n        texts.append(text)\n    return { \"text\" : texts, }\n\n# Apply formatting\nEOS_TOKEN = tokenizer.eos_token\ndataset = dataset.map(formatting_prompts_func, batched=True)\n\n# === NEW: Split Data for Evaluation ===\n# We use 90% for training and 10% for evaluation/testing\n# This allows us to calculate \"Validation Loss\" during training\ndataset_split = dataset.train_test_split(test_size=0.1)\ntrain_dataset = dataset_split[\"train\"]\neval_dataset = dataset_split[\"test\"]\n\nprint(f\"Training Samples: {len(train_dataset)}\")\nprint(f\"Evaluation Samples: {len(eval_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:03:51.132630Z","iopub.execute_input":"2026-01-10T15:03:51.132939Z","iopub.status.idle":"2026-01-10T15:03:51.235442Z","shell.execute_reply.started":"2026-01-10T15:03:51.132909Z","shell.execute_reply":"2026-01-10T15:03:51.234864Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3146 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cf9ddd0b8184a28becbf75b507c5b1c"}},"metadata":{}},{"name":"stdout","text":"Training Samples: 2831\nEvaluation Samples: 315\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Configure QLoRA Adapters","metadata":{}},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Rank. 16 is a good balance. 32/64 if you have a lot of data/compute.\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha = 16,\n    lora_dropout = 0, # 0 is optimized\n    bias = \"none\",    # \"none\" is optimized\n    use_gradient_checkpointing = \"unsloth\", # Crucial for long context/low VRAM\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:03:51.236377Z","iopub.execute_input":"2026-01-10T15:03:51.236688Z","iopub.status.idle":"2026-01-10T15:04:04.345487Z","shell.execute_reply.started":"2026-01-10T15:03:51.236647Z","shell.execute_reply":"2026-01-10T15:04:04.344881Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2026.1.2 patched 42 layers with 42 QKV layers, 42 O layers and 42 MLP layers.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Training (SFTTrainer)","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\n# 1. Split data (90% train, 10% test) to enable evaluation\ndataset_split = dataset.train_test_split(test_size=0.1)\ntrain_dataset = dataset_split[\"train\"]\neval_dataset = dataset_split[\"test\"]\n\n# 2. Configure Training\nsft_config = SFTConfig(\n    output_dir = \"outputs\",\n    max_seq_length = max_seq_length,\n    dataset_text_field = \"text\",\n    dataset_num_proc = 2,\n    packing = False,\n    per_device_train_batch_size = 2,\n    gradient_accumulation_steps = 4,\n    warmup_steps = 5,\n    max_steps = 100, \n    learning_rate = 2e-4,\n    fp16 = not is_bfloat16_supported(),\n    bf16 = is_bfloat16_supported(),\n    optim = \"adamw_8bit\",\n    weight_decay = 0.01,\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    eval_strategy = \"steps\",       \n    eval_steps = 20,               \n    per_device_eval_batch_size = 2,\n    logging_steps = 1,\n    report_to = \"none\",\n)\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_dataset,\n    eval_dataset = eval_dataset,   \n    args = sft_config,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:04:04.346475Z","iopub.execute_input":"2026-01-10T15:04:04.346773Z","iopub.status.idle":"2026-01-10T15:04:11.931006Z","shell.execute_reply.started":"2026-01-10T15:04:04.346735Z","shell.execute_reply":"2026-01-10T15:04:11.930283Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/2831 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1886a947fb347218c16979bf00990e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/315 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c85c1c4e558048bfbe80114ec83c7f03"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## Execute Training","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:04:11.932051Z","iopub.execute_input":"2026-01-10T15:04:11.932369Z","iopub.status.idle":"2026-01-10T15:21:48.686309Z","shell.execute_reply.started":"2026-01-10T15:04:11.932331Z","shell.execute_reply":"2026-01-10T15:21:48.685457Z"}},"outputs":[{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 2,831 | Num Epochs = 1 | Total steps = 100\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 54,018,048 of 9,295,724,032 (0.58% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 16:53, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.165400</td>\n      <td>0.962955</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.988300</td>\n      <td>0.878309</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.988200</td>\n      <td>0.837279</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.736100</td>\n      <td>0.821064</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.753000</td>\n      <td>0.811591</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Not an error, but Gemma2ForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Inference and Evaluation","metadata":{}},{"cell_type":"code","source":"import time\n\n# 1. Switch to inference mode\nFastLanguageModel.for_inference(model)\n\n# 2. Define varied test cases from your dataset\ntest_prompts = [\n    \"Jelaskan prinsip 'Nullum delictum nulla poena sine praevia lege poenali'.\",\n    \"Apa sanksi bagi pejabat yang menyalahgunakan wewenang?\",\n    \"Jelaskan perbedaan antara hukum perdata dan hukum pidana.\"\n]\n\n# 3. Advanced Generation Function\ndef generate_response(prompt, context=\"\"):\n    inputs = tokenizer(\n        [\n            alpaca_prompt.format(\n                prompt,\n                context,\n                \"\", # Output is empty for generation\n            )\n        ],\n        return_tensors=\"pt\"\n    ).to(\"cuda\")\n\n    start_time = time.time()\n    \n    # Advanced parameters for control\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=256,       \n        use_cache=True,\n        temperature=0.6,          # Lower (0.1-0.6) is better for factual/legal tasks\n        top_k=50,                 \n        top_p=0.9,                \n        repetition_penalty=1.1,   \n        do_sample=True            \n    )\n    \n    end_time = time.time()\n    \n    decoded_output = tokenizer.batch_decode(outputs)[0]\n    response = decoded_output.split(\"### Response:\\n\")[-1].replace(tokenizer.eos_token, \"\")\n    \n    # Calculate speed\n    num_tokens = len(outputs[0])\n    duration = end_time - start_time\n    tokens_per_sec = num_tokens / duration\n    \n    return response, tokens_per_sec\n\n# 4. Run Evaluation\nprint(\"=== Starting Advanced Evaluation ===\\n\")\n\nfor i, prompt in enumerate(test_prompts):\n    print(f\"Test Case {i+1}: {prompt}\")\n    response, speed = generate_response(prompt)\n    print(f\"Response:\\n{response}\")\n    print(f\"Speed: {speed:.2f} tokens/sec\")\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:21:48.687368Z","iopub.execute_input":"2026-01-10T15:21:48.687670Z","iopub.status.idle":"2026-01-10T15:22:02.754135Z","shell.execute_reply.started":"2026-01-10T15:21:48.687642Z","shell.execute_reply":"2026-01-10T15:22:02.753483Z"}},"outputs":[{"name":"stdout","text":"=== Starting Advanced Evaluation ===\n\nTest Case 1: Jelaskan prinsip 'Nullum delictum nulla poena sine praevia lege poenali'.\nResponse:\nTidak ada kejahatan tanpa hukuman kecuali dengan undang-undang yang berlaku sebelumnya (lex posterior).\nSpeed: 13.36 tokens/sec\n--------------------------------------------------\nTest Case 2: Apa sanksi bagi pejabat yang menyalahgunakan wewenang?\nResponse:\nPenjara 1-5 tahun dan/atau denda Rp 20 juta - Rp 4 miliar.\nSpeed: 28.47 tokens/sec\n--------------------------------------------------\nTest Case 3: Jelaskan perbedaan antara hukum perdata dan hukum pidana.\nResponse:\nHukum perdata mengatur sengketa antar individu atau badan usaha dengan tujuan mencari keadilan dalam bentuk ganti rugi. Hukum pidana mengatur perbuatan yang bertentangan dengan ketertiban umum untuk melindungi masyarakat dari kejahatan guna menghukum pelaku.\nSpeed: 19.42 tokens/sec\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Save the Model","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# 1. Login\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_token\")\nlogin(hf_token)\n\nrepo_name = \"bayhaqieee/gemma-2-9b-nlaw-gguf\" \n\nprint(\"=== STEP 1: Pushing GGUF to Hugging Face (This takes time & RAM) ===\")\n# We do this FIRST to ensure disk space is available for the merge process.\ntry:\n    model.save_pretrained_gguf(\n        repo_name, \n        tokenizer, \n        quantization_method = \"q4_k_m\",\n        token = hf_token\n    )\n    print(\"GGUF Pushed to Hugging Face successfully!\")\nexcept Exception as e:\n    print(f\"GGUF Push Failed: {e}\")\n\nprint(\"\\n Pushing Adapters (LoRA) to Hugging Face \")\n# This saves just the small learning files to your repo\nmodel.push_to_hub(repo_name, token=hf_token)\ntokenizer.push_to_hub(repo_name, token=hf_token)\nprint(\"Adapters Pushed to Hugging Face!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:22:02.755880Z","iopub.execute_input":"2026-01-10T15:22:02.756135Z","iopub.status.idle":"2026-01-10T15:22:14.556933Z","shell.execute_reply.started":"2026-01-10T15:22:02.756106Z","shell.execute_reply":"2026-01-10T15:22:14.556161Z"}},"outputs":[{"name":"stdout","text":"=== STEP 1: Pushing GGUF to Hugging Face (This takes time & RAM) ===\nUnsloth: Merging model weights to 16-bit format...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"414f4c254ce5448c9f7635efb824fb47"}},"metadata":{}},{"name":"stdout","text":"GGUF Push Failed: Failed to save/merge model: Unsloth: Failed saving locally - no disk space left. Uploading can work luckily! Use .push_to_hub instead.\n\n Pushing Adapters (LoRA) to Hugging Face \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/548 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57af591cdc444ee8a388cb28429a19a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08d25b12ce804f99b7e8f567f1051bd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3526e684f94b459d85bca468b1c8284f"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/bayhaqieee/gemma-2-9b-nlaw-gguf\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"328c30bce8a8487ea4439ebcb729d5c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1f94139f2e54987a6d9626989da834c"}},"metadata":{}},{"name":"stdout","text":"Adapters Pushed to Hugging Face!\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(\"\\nSaving Adapters Locally (Kaggle)\")\nlocal_folder = \"gemma-2-9b-nlaw_adapter\"\nmodel.save_pretrained(local_folder)\ntokenizer.save_pretrained(local_folder)\nprint(f\"Adapters saved locally to folder: {local_folder}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T15:22:14.557943Z","iopub.execute_input":"2026-01-10T15:22:14.558199Z","iopub.status.idle":"2026-01-10T15:22:15.458727Z","shell.execute_reply.started":"2026-01-10T15:22:14.558173Z","shell.execute_reply":"2026-01-10T15:22:15.457915Z"}},"outputs":[{"name":"stdout","text":"\nSaving Adapters Locally (Kaggle)\nAdapters saved locally to folder: gemma-2-9b-nlaw_adapter\n","output_type":"stream"}],"execution_count":15}]}